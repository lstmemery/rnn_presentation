<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Fast.AI: Lesson 6</title>
    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="reveal.js/css/theme/moon.css">
    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="css/solarized-dark.css">
    <!-- Printing and PDF exports -->
    <script>
    var link = document.createElement('link');
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <h1>Fast.ai Lesson 6 Review</h1>
                <p>Matthew Emery [<a href="https://github.com/lstmemery/">@lstmemery</a>]</p>
                </section>
            <section>
                <ol>
                    <li>Use cases for RNNs</li>
                    <li>Three Character Model</li>
                    <li>Nth Character Model</li>
                    <li>Sequential RNNs</li>
                    <li>Stateful RNNs</li>
                    <li>RNN in Theano</li>
                </ol>
                <aside class="notes">Table of Contents Here</aside>
            </section>
            <section>
                    <h2>Some changes to Jeremy's Code</h2>
                    <p>Migrated from Keras 1 to 2</p>
                    <p>Migrated from Python 2 to 3</p>
                    <p>Using a Tensorflow backend (instead of Theano)</p>
                    <p>Only important difference: Had to remove extra dimensions for some training</p>
            </section>
            <section>
                    <h2>Use cases for RNNs</h2>
                    <ul>
                        <li>Anything that requires knowledge of long term structure</li>
                        <li>Text data</li>
						<li>Sequential data</li>
                        <li>Time series</li>
                    </ul>
            </section>
            <section>
                <section>
                    <h1>Three Character Model</h1>
                </section>
                <section>
                    <img src="img/3char_model.png">
                </section>
            	<section>
                <h2>Mapping characters to integers</h2>
                <pre><code data-trim data-noescape class="python">
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))
				</code></pre>
                <aside class="notes">This creates a mapping of characters to integers. RNNs take numbers, not characters</aside>
		        </section>
                <section>
                    <h2>Inputs and output</h2>
                                    <pre><code data-trim data-noescape class="python">
cs=3
c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, cs)]
c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]
c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]
c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]
                        </code></pre>
                    <p>This creates a list of numbers.</p>
                    <p>We need a matrix. So we create an embedding</p>
                </section>
		        <section>
		            <h2>Embedding</h2>
		            <p>n_fac is a hyperparameter</p>
                <pre><code data-trim data-noescape class="python">
def embedding_input(name, n_in, n_out):
    inp = Input(shape=(1,), dtype='int64', name=name)
    emb = Embedding(n_in, n_out, input_length=1)(inp)
    return inp, Flatten()(emb)
                </code></pre>
                <p>Parameters: 3570</p>
                <aside class="notes">Anyone know why int64 was used?
                Embedding layer is size (85 [Number of tokens] * 42[Number of latent factors]) = 3570 parameters</aside>
		        </section>
                <section>
                    <img src="img/3char_model.png">
                </section>
                <section>
                    <h2>Input to Hidden Layer</h2>
                    <pre><code data-trim data-noescape class="python">
n_hidden = 256
dense_in = Dense(n_hidden, activation='relu')
c1_hidden = dense_in(c1)
                    </code></pre>
                    <p>Parameters: 11008</p>
                    <aside class="notes">42[latent] * 256[hidden] + 256[biases]</aside>
                </section>
                <section>
                    <h2>Hidden to Hidden Layer</h2>
                                        <pre><code data-trim data-noescape class="python">
from keras.layers.merge import add as merge_add
dense_hidden = Dense(n_hidden, activation='relu')
c2_dense = dense_in(c2)
hidden_2 = dense_hidden(c1_hidden)
c2_hidden = merge_add([c2_dense, hidden_2])  # Element-wise sum
                    </code></pre>
                    <p>Parameters: 65792</p>
                    <aside class="notes">256[previous latent] * 256[new latent] + 256[biases]</aside>
                </section>
                <section>
                    <h2>Hidden to Output Layer</h2>
                                        <pre><code data-trim data-noescape class="python">
c3_dense = dense_in(c3)
hidden_3 = dense_hidden(c2_hidden)
c3_hidden = merge_add([c3_dense, hidden_3])
dense_out = Dense(vocab_size, activation='softmax')
c4_out = dense_out(c3_hidden)
                    </code></pre>
                    <p>Parameters: 21845</p>
                    <aside class="notes">256[previous latent] * 85[output] + 85[biases]</aside>
                </section>
            </section>
            <section>
                <section>
                    <h1>Nth Character Model</h1>
                </section>
                <section>
                    <h2>RNN forms:</h2>
                    <ol>
                        <li>Unrolled Form (previous example)</li>
                        <li>Recurrent Form (next example)</li>
                    </ol>
                    <p>Note: Jeremy mentioned that Tensorflow does not have the ability to make recurrent form RNNs.</p>
                    <p>This is no longer true. Use `tf.nn.dynamic_rnn`.</p>
                    <aside class="notes">CNTK does not support dropout on recurrent form RNNs</aside> 
                </section>
                <section>
                    <img style="height: 40%; width: 40%" src="img/nth_char_model.png">
                    <pre><code data-trim data-noescape class="python">
dense_in = Dense(n_hidden, activation='relu')
dense_hidden = Dense(n_hidden, activation='relu', 
                     kernel_initializer="identity")
dense_out = Dense(vocab_size, activation='softmax')
                    </code></pre>

                </section>

                <section>
                    <h2>Combining layers</h2>
                    <pre><code data-trim data-noescape class="python">
hidden = dense_in(c_ins[0][1])
for i in range(1,cs):
    c_dense = dense_in(c_ins[i][1])
    hidden = dense_hidden(hidden)
    hidden = merge_add([c_dense, hidden])
c_out = dense_out(hidden)
                    </code></pre>
                    <p>This is common architecture. So Keras already has a version of it.</p>
                </section>
                <section>
                    <h2>Keras: SimpleRNN</h2>
                                        <pre><code data-trim data-noescape class="python">
model=Sequential([
        Embedding(vocab_size, n_fac, input_length=cs),
        SimpleRNN(n_hidden, activation='relu',
                  recurrent_initializer='identity'),
        Dense(vocab_size, activation='softmax')
    ])
                    </code></pre>
                    <p>By setting recurrent_initializer = identity means do nothing by default</p>
                    <p>Parameters of Dense layer: 22102</p>
                    <p>Parameters of SimpleRNN: 76544</p>
                    <aside class="notes">Dense: 256[Input] * 86[Output] = 22102</aside>
                    <aside class="notes">SimpleRNN: 256*256[Hidden] + (42 + 1)[Input] * 256 = 76544</aside>
                </section>
            </section>
            <section>
                <section><h1>Sequential RNNs</h1></section>
                <section>
                    <h2>Previously...</h2>
                    <img style="height: 70%; width: 70%" src="img/nth_char_model.png">
                </section>
                <section>
                    <h2>Now</h2>
                    <img style="height: 70%; width: 70%" src="img/seq_rnn_model.png">
                </section>
                <section>
                    <p>The output of one step is the input of the next step</p>
                    <pre><code data-trim data-noescape class="python">
c_in_dat = [[idx[i+n] for i in range(0, len(idx) - 1 - cs, cs)]
            for n in range(cs)]
c_out_dat = [[idx[i+n] for i in range(1, len(idx) - cs, cs)] 
            for n in range(cs)] # This changed
                    </code></pre>
                </section>
                <section>
                    <h2>The input also changes</h2>
                    <pre><code data-trim data-noescape class="python">
inp1 = Input(shape=(n_fac,), name='zeros')
hidden = dense_in(inp1)
                    </code></pre>
                    <aside class="notes">We want the input to be used in each loop, but need to initialize
                        to something.</aside>
                </section>
                <section>
                    <pre><code data-trim data-noescape class="python">
outs = []

for i in range(cs):
    c_dense = dense_in(c_ins[i][1])
    hidden = dense_hidden(hidden)
    hidden = merge_add([c_dense, hidden], mode='sum')
    # every layer now has an output
    outs.append(dense_out(hidden))
                    </code></pre>
                    <p>Eight outputs means eight binary cross entropies to manage</p>
                    <aside class="notes">The first character will always be difficult to predict, because there is no info.</aside>                 
                </section>
                <section>
                    <h2>This is easy in Keras</h2>
                    <pre><code data-trim data-noescape class="python">
model=Sequential([
        Embedding(vocab_size, n_fac, input_length=cs),
        SimpleRNN(n_hidden, 
                  return_sequences=True,
                  activation='relu', 
                  recurrent_initializer='identity'),
        TimeDistributed(Dense(vocab_size, activation='softmax')),
    ])
                    </code></pre>
                    <p>Two differences</p>
                    <ol>
                        <li>return_sequences=True</li>
                        <li>TimeDistributed wrapped around Dense</li>
                    </ol>
                    <aside class="notes">TimeDistributed is a wrapper that applies a Dense layer to each time step.</aside>
                </section>

            </section>
            <section>
                <section>
                    <h1>Stateful RNNs</h1>
                </section>
                <section>
                    <ul>
                        <li>We want to model long-term dependencies</li>
                        <li>The RNN needs to be aware of ALL of the previous sequence</li>
                        <li>This was impossible until the creation of Long-Short Term Memory</li>
                        <li>An LSTM replaces the hidden-to-hidden matrix with another neural net</li>
                        <li>LSTMs are difficult to parallelize. Training is slower</li>
                    </ul>
                <aside class="notes">LSTM Gates: Forget, Input, Output, sometimes bias?</aside>
                </section>
                <section>
                <h2>First Model</h2>
                    <pre><code data-trim data-noescape class="python">
model=Sequential([
        Embedding(vocab_size, n_fac, input_length=cs,
                  batch_input_shape=(bs, 8)),
        BatchNormalization(),
        LSTM(n_hidden, return_sequences=True, stateful=True),
        TimeDistributed(Dense(vocab_size, activation='softmax')),
    ])
                    </code></pre>
                    <p>Batch Normalization was required to get this model running</p>
                    <p>BatchNorm Parameters: 168</p>
                    <p>LSTM Parameters: 306176</p>
                    <aside class="notes">
                        <p>84[Input] * 42[Latent Factors] = 3612</p>
                        <p>42[Input] * 2 [Parameters] * 2[Dimensions] =168</p>
                        <p>4 * ((42[Input] + 1)*256[Output] + 256**2) = 306176</p>
                        <p>22102 = 256[Input] * [86]Output</p>
                    </aside>
                </section>
                <section>
                <h2>Stacked RNN</h2>
                <img style="height: 70%; width: 70%" src="img/stacked_model.png">
                </section>
                <section>
            <pre><code data-trim data-noescape class="python">
model=Sequential([
        Embedding(vocab_size, n_fac, input_length=maxlen),
        LSTM(512, recurrent_dropout=0.2, return_sequences=True, 
             dropout=0.2, implementation=2, input_shape=(n_fac,)),
        Dropout(0.2),
        LSTM(512, dropout=0.2, recurrent_dropout=0.2, 
             return_sequences=True),
        Dropout(0.2),
        TimeDistributed(Dense(vocab_size)),
        Activation('softmax')
    ])                    
            </code></pre>
            <p>This will take a long time to train but gives the best results we've seen so far</p>
                </section>
            </section>
            <section>
                <section><h1>Theano RNN</h1></section>
                <section>
                <p>Sparse Categorical Cross-entropy just converts integer to a one-hot encoding implicitly</p>
                <p>We to make this explicit for Theano</p>
                    <pre><code data-trim data-noescape class="python">
model=Sequential([
        SimpleRNN(n_hidden, return_sequences=True,
                  input_shape=(cs, vocab_size),
                  activation='relu',
                  recurrent_initializer="identity"),
        TimeDistributed(Dense(vocab_size, activation='softmax')),
    ])
model.compile(loss='categorical_crossentropy', optimizer=Adam())
                    </code></pre>
                    <aside class="notes">Not strictly true, we don't use the Adam Optimizer</aside>
                </section>
                <section>
                    <img src="img/seq_rnn_model.png">
                </section>
                <section>
                    <h2>Theano</h2>
                    <ul>
                        <li>Manipulates tensors in the GPU</li>
                        <li>Automatic Differentiation</li>
                        <li>Need to declare upfront for computational graph</li>
                        <li>Used shared to claim an array for theano</li>
                        <li>Breaks CUDA when you try upgrade it the day before a presentation</li>
                    </ul>
                </section>
                <section>
                    <h2>Initialization</h2>
                                        <pre><code data-trim data-noescape class="python">
def init_wgts(rows, cols):
    scale = math.sqrt(2 / rows)
    return shared(normal(scale=scale,
                         size=(rows, cols)).astype(np.float32))
def init_bias(rows):
    return shared(np.zeros(rows, dtype=np.float32))
                    </code></pre>
                    <aside class="notes">init_wgts is using Glorot initialization</aside>
                </section>
                <section>
                    <h2>Initialization</h2>
                                        <pre><code data-trim data-noescape class="python">
def init_wgts(rows, cols):
    scale = math.sqrt(2 / rows)
    return shared(normal(scale=scale,
                         size=(rows, cols)).astype(np.float32))

def init_bias(rows):
    return shared(np.zeros(rows, dtype=np.float32))

def wgts_and_bias(n_in, n_out):
    return init_wgts(n_in, n_out), init_bias(n_out)

def id_and_bias(n):
    return shared(np.eye(n, dtype=np.float32)), init_bias(n)
                    </code></pre>
                    <aside class="notes">init_wgts is using Glorot initialization</aside>
                </section>
                <section>
                    <h2>Announcing to Theano</h2>
                    <pre><code data-trim data-noescape class="python">
t_inp = T.matrix('inp', dtype="float32")
t_outp = T.matrix('outp', dtype="float32")
t_h0 = T.vector('h0', dtype="float32")
lr = T.scalar('lr', dtype="float32")

all_args = [t_h0, t_inp, t_outp, lr]

W_h = id_and_bias(n_hidden)             # Hidden-to-hidden
W_x = wgts_and_bias(n_input, n_hidden)  # Input-to-hidden
W_y = wgts_and_bias(n_hidden, n_output) # Hidden-to-output
w_all = list(chain.from_iterable([W_h, W_x, W_y]))
                    </code></pre>
                    <aside class="notes">Announcing to Theano. Had to include dtype to stop errors.</aside>
                </section>
                <section>
                    <h2>Scanning</h2>
                    <pre><code data-trim data-noescape class="python">
def step(x, h, W_h, b_h, W_x, b_x, W_y, b_y):
    # Calculate the hidden activations
    h = nnet.relu(T.dot(x, W_x) + b_x + T.dot(h, W_h) + b_h)
    # Calculate the output activations
    y = nnet.softmax(T.dot(h, W_y) + b_y)
    # Return both (the 'Flatten()' is to work around a theano bug)
    return h, T.flatten(y, 1)

[v_h, v_y], _ = theano.scan(step, sequences=t_inp,
                            outputs_info=[t_h0, None],
                            non_sequences=w_all)
                    </code></pre>
                    <aside class="notes">I believe the None represents no prior info about output</aside>
                </section>
                <section>
                    <h2>Gradients</h2>
                    <pre><code data-trim data-noescape class="python">
error = nnet.categorical_crossentropy(v_y, t_outp).sum()
g_all = T.grad(error, w_all)

def upd_dict(wgts, grads, lr):
    return OrderedDict({w: w - g * lr for (w, g)
                        in zip(wgts, grads)})
                        # This is gradient descent!

upd = upd_dict(w_all, g_all, lr)

fn = theano.function(all_args, error, updates=upd,
                     allow_input_downcast=True)
                    </code></pre>
                    <aside class="notes">All of the gradient work happens here!</aside>
                </section>
                <section>
                    <h2>Updates</h2>
                    <pre><code data-trim data-noescape class="python">
err=0.0; l_rate=0.01
for i in range(len(X)):
    err += fn(np.zeros(n_hidden), X[i], Y[i], l_rate)
    if i % 1000 == 999:
        print ("Error:{:.3f}".format(err / 1000))
        err=0.0
                    </code></pre>
                    <aside class="notes">This should make the previous slide a little more clear</aside>
                </section>
                <section>
                    <h2>Answers</h2>
                    <pre><code data-trim data-noescape class="python">
f_y = theano.function([t_h0, t_inp], v_y,
                      allow_input_downcast=True)

pred = np.argmax(f_y(np.zeros(n_hidden), X[6]), axis=1)
act = np.argmax(X[6], axis=1)

[indices_char[o] for o in act]
[indices_char[o] for o in pred]
                    </code></pre>
                    <aside class="notes">This did not do well</aside>
                </section>

            </section>
            <section><h1>Questions?</h1></section>
        </div>
    </div>
    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.js"></script>
    <script>
    // More info about config & dependencies:
    // - https://github.com/hakimel/reveal.js#configuration
    // - https://github.com/hakimel/reveal.js#dependencies
    Reveal.initialize({
        dependencies: [
            { src: 'reveal.js/plugin/markdown/marked.js' },
            { src: 'reveal.js/plugin/markdown/markdown.js' },
            { src: 'reveal.js/plugin/notes/notes.js', async: true },
            { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
    });
    </script>
</body>

</html>
